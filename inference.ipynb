{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from PIL import Image\n",
    "from utils.options import Options\n",
    "from utils.model_utils import load_models\n",
    "from scripts.bald_proxy import BaldProxy\n",
    "from scripts.align_proxy import AlignProxy\n",
    "from utils.get_e4e import E4EEncoder\n",
    "from utils.get_FS import FSEncoder\n",
    "from scripts.blend_proxy import hairstyle_feature_blending\n",
    "from utils.image_utils import show_results\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ResNet ArcFace\n"
     ]
    }
   ],
   "source": [
    "opts = Options().parse(jupyter=True)\n",
    "generator, latent_avg, seg_net, kp_extractor, bald_mapper, alpha, bald_editor, sfe_encoder, align_mapper = load_models(opts)\n",
    "e4e_encoder = E4EEncoder(opts.e4e_path, latent_avg, device=opts.device)\n",
    "fs_encoder = FSEncoder(generator, latent_avg, sfe_encoder)\n",
    "bald_proxy = BaldProxy(opts, generator, bald_mapper, alpha, seg_net, bald_editor, fs_encoder)\n",
    "align_proxy = AlignProxy(opts, generator, seg_net, kp_extractor, latent_avg, align_mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up PyTorch plugin \"bias_act_plugin\"...\n",
      "Using /root/.cache/torch_extensions/py310_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu113/bias_act_plugin/build.ninja...\n",
      "Building extension module bias_act_plugin...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include/TH -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/autodl-tmp/conda/envs/sfe/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' --use_fast_math -std=c++14 -c /root/autodl-tmp/lijijie/GlamourGraft_demo/models/bald_proxy/torch_utils/ops/bias_act.cu -o bias_act.cuda.o \n",
      "[2/3] c++ -MMD -MF bias_act.o.d -DTORCH_EXTENSION_NAME=bias_act_plugin -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1013\\\" -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include/TH -isystem /root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /root/autodl-tmp/conda/envs/sfe/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /root/autodl-tmp/lijijie/GlamourGraft_demo/models/bald_proxy/torch_utils/ops/bias_act.cpp -o bias_act.o \n",
      "[3/3] c++ bias_act.o bias_act.cuda.o -shared -L/root/autodl-tmp/conda/envs/sfe/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o bias_act_plugin.so\n",
      "Loading extension module bias_act_plugin...\n",
      "Done setting up PyTorch plugin \"bias_act_plugin\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"edited_img, ref_rotated_w = align_proxy(ref_img, bald_img, w_bald, ref_e4e_latent, src_pre, ref_pre)\\nstyle_output = hairstyle_feature_blending(generator, seg_net, src_sfe_w, F_bald_edited, ref_rotated_w, bald_img)\\nstyle_output.save(os.path.join(opts.edited_img_dir, f'{src_pre}_{ref_pre}_edited.jpg'))\\nshow_results(src_img, ref_img, bald_img, style_output)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_name = '000080.jpg'\n",
    "ref_name = '089255.jpg'\n",
    "src_pre = src_name.split('.')[0]\n",
    "ref_pre = ref_name.split('.')[0]\n",
    "src_path = os.path.join(opts.src_img_dir, src_name)\n",
    "ref_path = os.path.join(opts.ref_img_dir, ref_name)\n",
    "src_img = Image.open(src_path).convert('RGB')\n",
    "ref_img = Image.open(ref_path).convert('RGB')\n",
    "src_e4e_latent = e4e_encoder.get_e4e(src_img) \n",
    "inversion_img, src_sfe_w, src_sfe_F, _ = fs_encoder.get_FS(src_img) \n",
    "src_sfe_w = src_sfe_w.to(opts.device)\n",
    "src_sfe_F = src_sfe_F.to(opts.device)\n",
    "ref_e4e_latent = e4e_encoder.get_e4e(ref_img) \n",
    "src_sfe_w, w_bald, bald_feature, F_bald_edited, bald_img = bald_proxy(src_img, src_e4e_latent, src_sfe_w, src_sfe_F, possion=True)\n",
    "edited_img, ref_rotated_w = align_proxy(ref_img, bald_img, w_bald, ref_e4e_latent, src_pre, ref_pre)\n",
    "style_output = hairstyle_feature_blending(generator, seg_net, src_sfe_w, F_bald_edited, ref_rotated_w, bald_img)\n",
    "style_output.save(os.path.join(opts.edited_img_dir, f'{src_pre}_{ref_pre}_edited.jpg'))\n",
    "show_results(src_img, ref_img, bald_img, style_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
